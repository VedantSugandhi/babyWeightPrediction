{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 1\n",
    "* CSCI-5930 ML Fall 2023  (Be sure to discard which section you are not enrolled)\n",
    "* Author: Vedant Sugandhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# First load the dataset into pandas dataframe\n",
    "full_dataset = pd.read_csv('dataset/baby-weights-dataset.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : (10 points)\n",
    "* Summarize the `baby-weights-dataset.csv` dataset in the following way:\n",
    "```\n",
    "total number of rows = a\n",
    "total number of columns = b\n",
    "number of columns having non-numeric values = c\n",
    "columns with missing values = [ (d1, e1)  (d2, e2), ... ]\n",
    "Father's education summary =  f +/- g \n",
    "```\n",
    "  \n",
    "where, \n",
    "* `a` is total number of rows in the dataset.\n",
    "* `b` is total number of columns in the dataset.\n",
    "* `c` is number of columns having non-numeric values.\n",
    "* $(d_i, e_i)$ (i.e., a pair/tuple entry) represents column name ($d_i$) and number of missing values present in that column ($e_i$). If number of missing values in a column is zero (0), you do not need to list it. Please sort the tuple entries in descending order of $e_i$ values.\n",
    "* `f` and `g` are average and standard deviation of father's education years among the data samples respectively. Please print the way it is shown above. Also, print the percentages in 2 decimal places after the decimal point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows = 101400\n",
      "total number of columns = 37\n",
      "Number of columns having non-numeric values= 2\n",
      "Number of missing values in each column:\n",
      "CIGNUM      1\n",
      "GAINED      1\n",
      "HYDRAM      1\n",
      "FEDUC       1\n",
      "WEEKS       1\n",
      "HYPERPR     0\n",
      "ACLUNG      0\n",
      "DIABETES    0\n",
      "HERPES      0\n",
      "HEMOGLOB    0\n",
      "HYPERCH     0\n",
      "ECLAMP      0\n",
      "ANEMIA      0\n",
      "CERVIX      0\n",
      "PINFANT     0\n",
      "PRETERM     0\n",
      "RENAL       0\n",
      "RHSEN       0\n",
      "UTERINE     0\n",
      "CARDIAC     0\n",
      "ID          0\n",
      "DRINKNUM    0\n",
      "SEX         0\n",
      "HISPDAD     0\n",
      "HISPMOM     0\n",
      "RACEDAD     0\n",
      "RACEMOM     0\n",
      "LOUTCOME    0\n",
      "TERMS       0\n",
      "BDEAD       0\n",
      "TOTALP      0\n",
      "MEDUC       0\n",
      "MAGE        0\n",
      "VISITS      0\n",
      "FAGE        0\n",
      "MARITAL     0\n",
      "BWEIGHT     0\n",
      "dtype: int64\n",
      "Father's education summary = '12.93'% +/- 2.93%\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "#For calculating total no of rows\n",
    "rows = full_dataset.count()[0]\n",
    "print(\"total number of rows =\", rows)\n",
    "#For calculating total no of cols\n",
    "cols = len(full_dataset.columns)\n",
    "print(\"total number of columns =\",cols)\n",
    "# For calculating Number of columns having non-numeric values\n",
    "non_numeric_columns = full_dataset.select_dtypes(exclude=['number'])\n",
    "num = len(non_numeric_columns.columns)\n",
    "print(f\"Number of columns having non-numeric values= {num}\")\n",
    "# For calculating Number of missing values\n",
    "missing_values_per_column = full_dataset.isnull().sum()\n",
    "missing_values_per_column = missing_values_per_column.sort_values(ascending=False)\n",
    "print(\"Number of missing values in each column:\")\n",
    "print(missing_values_per_column)\n",
    "# For calculating Average & standard deviation\n",
    "column_name = 'FEDUC'\n",
    "average = round(full_dataset[column_name].mean(),2)\n",
    "std_deviation = round(full_dataset[column_name].std(),2)\n",
    "print(f\"Father's education summary = '{average}'% +/- {std_deviation}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: \n",
    "Separate the full_dataset into two parts: X and y, where X denotes the input matrix containing only the input (i.e., independent explanatory) variables, and y denotes the target variable containing only the target values for exactly the same number of samples in the given full_dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: Your code goes here\n",
    "target_column = 'BWEIGHT'\n",
    "X = full_dataset.drop(columns=[target_column])  # X contains all columns except the target column\n",
    "y = full_dataset[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3:\n",
    "* Given X representing the input matrix from the full_dataset, y being the target vector (the rightmost column of the full_dataset), obtained from Task 2: \n",
    "* randomly split the (X,y) dataset into 75% for training and 25% for testing using the library function from the library [sklearn.model_selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) . Please pass to the train_test_split function an additional argument random_state=12346.\n",
    "* Store the 4 splits as X_train, X_test, y_train, y_test respectively.\n",
    "* Save the ID column for X_train and X_test into ID_train and ID_test as list variable.\n",
    "* Now, drop the ID columns from both X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@TODO: Your code goes here\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.75, test_size=0.25, random_state=12346)\n",
    "ID_column_name = 'ID'\n",
    "\n",
    "# Create ID_train and ID_test as list variables\n",
    "ID_train = X_train[ID_column_name].tolist()\n",
    "ID_test = X_test[ID_column_name].tolist()\n",
    "X_train = X_train.drop(columns=[ID_column_name])\n",
    "X_test = X_test.drop(columns=[ID_column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: \n",
    "Given the training dataset (X_train, y_train), save as X_train_ohe after replacing all the non-numeric variables (i.e., categorical variables) with numeric encoding. Please consider using the \"One-hot encoding\" scheme i.e., introducing dummy variables. A brief description of the scheme can be found in the [DUMMY-variables.note.txt](DUMMY-variables.note.txt) file\n",
    "* Use the same encoder to perform onehotencoding on the X_test dataset and save the result as X_test_ohe.\n",
    "* Print the column names of X_test_ohe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names of X_test_ohe:\n",
      "Index(['SEX', 'MARITAL', 'FAGE', 'GAINED', 'VISITS', 'MAGE', 'FEDUC', 'MEDUC',\n",
      "       'TOTALP', 'BDEAD', 'TERMS', 'LOUTCOME', 'WEEKS', 'RACEMOM', 'RACEDAD',\n",
      "       'CIGNUM', 'DRINKNUM', 'ANEMIA', 'CARDIAC', 'ACLUNG', 'DIABETES',\n",
      "       'HERPES', 'HYDRAM', 'HEMOGLOB', 'HYPERCH', 'HYPERPR', 'ECLAMP',\n",
      "       'CERVIX', 'PINFANT', 'PRETERM', 'RENAL', 'RHSEN', 'UTERINE',\n",
      "       'HISPMOM_C', 'HISPMOM_M', 'HISPMOM_N', 'HISPMOM_O', 'HISPMOM_P',\n",
      "       'HISPMOM_S', 'HISPMOM_U', 'HISPDAD_C', 'HISPDAD_M', 'HISPDAD_N',\n",
      "       'HISPDAD_O', 'HISPDAD_P', 'HISPDAD_S', 'HISPDAD_U'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_train_ohe = pd.get_dummies(X_train)\n",
    "X_test_ohe = pd.get_dummies(X_test)\n",
    "\n",
    "cc = X_train.select_dtypes(exclude=['number']).columns\n",
    "#print(cc)\n",
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(), cc),\n",
    "    remainder='passthrough')\n",
    "\n",
    "#ohe_dataset = pd.DataFrame(column_trans.fit_transform(X_train),columns=column_trans.get_feature_names_out())\n",
    "#print(len(ohe_dataset))\n",
    "#print(ohe_dataset)\n",
    "# Print the column names of X_test_ohe\n",
    "\n",
    "print(\"Column names of X_test_ohe:\")\n",
    "print(X_test_ohe.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5: \n",
    "* Given the X_train_ohe (Onehot encoded Pandas Dataframe from Task 4), check if there are missing values, and if yes, count how many, and impute the missing values with corresponding mean values. \n",
    "* Finally, print the counting result as a Pandas dataframe named \"missing_counts\" having 2 columns {variable_name,num_of_missing_values).  Please make sure that the result lists all the input variables in the given dataset. \n",
    "* Now, impute the missing values by mean of the respective variable and save the revised dataframe as X_train_ohe_imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values have been imputed with mean values.\n",
      "missing_counts DataFrame:\n",
      "          variable_name  num_of_missing_values\n",
      "SEX                 SEX                      0\n",
      "MARITAL         MARITAL                      0\n",
      "FAGE               FAGE                      0\n",
      "GAINED           GAINED                      1\n",
      "VISITS           VISITS                      0\n",
      "MAGE               MAGE                      0\n",
      "FEDUC             FEDUC                      1\n",
      "MEDUC             MEDUC                      0\n",
      "TOTALP           TOTALP                      0\n",
      "BDEAD             BDEAD                      0\n",
      "TERMS             TERMS                      0\n",
      "LOUTCOME       LOUTCOME                      0\n",
      "WEEKS             WEEKS                      0\n",
      "RACEMOM         RACEMOM                      0\n",
      "RACEDAD         RACEDAD                      0\n",
      "CIGNUM           CIGNUM                      0\n",
      "DRINKNUM       DRINKNUM                      0\n",
      "ANEMIA           ANEMIA                      0\n",
      "CARDIAC         CARDIAC                      0\n",
      "ACLUNG           ACLUNG                      0\n",
      "DIABETES       DIABETES                      0\n",
      "HERPES           HERPES                      0\n",
      "HYDRAM           HYDRAM                      1\n",
      "HEMOGLOB       HEMOGLOB                      0\n",
      "HYPERCH         HYPERCH                      0\n",
      "HYPERPR         HYPERPR                      0\n",
      "ECLAMP           ECLAMP                      0\n",
      "CERVIX           CERVIX                      0\n",
      "PINFANT         PINFANT                      0\n",
      "PRETERM         PRETERM                      0\n",
      "RENAL             RENAL                      0\n",
      "RHSEN             RHSEN                      0\n",
      "UTERINE         UTERINE                      0\n",
      "HISPMOM_C     HISPMOM_C                      0\n",
      "HISPMOM_M     HISPMOM_M                      0\n",
      "HISPMOM_N     HISPMOM_N                      0\n",
      "HISPMOM_O     HISPMOM_O                      0\n",
      "HISPMOM_P     HISPMOM_P                      0\n",
      "HISPMOM_S     HISPMOM_S                      0\n",
      "HISPMOM_U     HISPMOM_U                      0\n",
      "HISPDAD_C     HISPDAD_C                      0\n",
      "HISPDAD_M     HISPDAD_M                      0\n",
      "HISPDAD_N     HISPDAD_N                      0\n",
      "HISPDAD_O     HISPDAD_O                      0\n",
      "HISPDAD_P     HISPDAD_P                      0\n",
      "HISPDAD_S     HISPDAD_S                      0\n",
      "HISPDAD_U     HISPDAD_U                      0\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a DataFrame to store the missing value counts\n",
    "missing_counts = pd.DataFrame({'variable_name': X_train_ohe.columns, 'num_of_missing_values': X_train_ohe.isnull().sum()})\n",
    "\n",
    "# Check if there are any missing values to impute\n",
    "if missing_counts['num_of_missing_values'].sum() > 0:\n",
    "    # Create an instance of the SimpleImputer with 'mean' strategy\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "    # Fit and transform the imputer on X_train_ohe\n",
    "    X_train_ohe_imputed = pd.DataFrame(imputer.fit_transform(X_train_ohe), columns=X_train_ohe.columns)\n",
    "    \n",
    "    # Check if there are still missing values (there shouldn't be)\n",
    "    remaining_missing = X_train_ohe_imputed.isnull().sum().sum()\n",
    "    \n",
    "    if remaining_missing == 0:\n",
    "        print(\"Missing values have been imputed with mean values.\")\n",
    "    else:\n",
    "        print(\"There are still missing values after imputation.\")\n",
    "else:\n",
    "    print(\"No missing values in X_train_ohe.\")\n",
    "\n",
    "# Print the missing_counts DataFrame\n",
    "print(\"missing_counts DataFrame:\")\n",
    "print(missing_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6: \n",
    "* Given a X_train_ohe_imputed (Pandas dataframe from Task 5) where all the categorical variables are already replaced with numeric values, print a list of top 20 highly correlated variables with respect to the target variable, and save the result as a Pandas dataframe named top20_df with 2 columns {variable,corr_score}. \n",
    "* Here, the corr_score between a variable x and the target variable y needs to be computed using the Pearson Correlation Coefficient (PCC). Please note, PCC ranges between -1 to +1. PCC score 0 means no correlation, while value towards +1 and -1 represent positive and negative correlations respectively. For instance, PCC=0.8 and PCC=-0.8 tell similar strength positive and negative correlations between the two subject variables.\n",
    "* Please do not include BWEIGHT in the top20_df list of top 20 correlated variable list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEX          0.002216\n",
      "MARITAL      0.003248\n",
      "FAGE         0.000868\n",
      "GAINED       0.009136\n",
      "VISITS       0.004913\n",
      "MAGE         0.002319\n",
      "FEDUC        0.004521\n",
      "MEDUC        0.007019\n",
      "TOTALP       0.002970\n",
      "BDEAD        0.002014\n",
      "TERMS        0.007409\n",
      "LOUTCOME     0.004958\n",
      "WEEKS        0.004020\n",
      "RACEMOM      0.000918\n",
      "RACEDAD      0.000259\n",
      "CIGNUM       0.005689\n",
      "DRINKNUM     0.006667\n",
      "ANEMIA       0.003884\n",
      "CARDIAC      0.000466\n",
      "ACLUNG       0.003360\n",
      "DIABETES     0.005237\n",
      "HERPES       0.004158\n",
      "HYDRAM       0.002099\n",
      "HEMOGLOB     0.005172\n",
      "HYPERCH      0.004707\n",
      "HYPERPR      0.003434\n",
      "ECLAMP       0.003288\n",
      "CERVIX       0.007630\n",
      "PINFANT      0.007952\n",
      "PRETERM      0.006375\n",
      "RENAL        0.001007\n",
      "RHSEN        0.002081\n",
      "UTERINE      0.000525\n",
      "HISPMOM_C    0.003199\n",
      "HISPMOM_M    0.009118\n",
      "HISPMOM_N    0.010705\n",
      "HISPMOM_O    0.001458\n",
      "HISPMOM_P    0.000884\n",
      "HISPMOM_S    0.006744\n",
      "HISPMOM_U    0.003924\n",
      "HISPDAD_C    0.002994\n",
      "HISPDAD_M    0.004629\n",
      "HISPDAD_N    0.005365\n",
      "HISPDAD_O    0.011512\n",
      "HISPDAD_P    0.006235\n",
      "HISPDAD_S    0.003090\n",
      "HISPDAD_U    0.000254\n",
      "dtype: float64\n",
      "List of top 20 correlated variables (excluding 'BWEIGHT'):\n",
      "     variable  corr_score\n",
      "0   HISPDAD_O    0.011512\n",
      "1   HISPMOM_N    0.010705\n",
      "2      GAINED    0.009136\n",
      "3   HISPMOM_M    0.009118\n",
      "4     PINFANT    0.007952\n",
      "5      CERVIX    0.007630\n",
      "6       TERMS    0.007409\n",
      "7       MEDUC    0.007019\n",
      "8   HISPMOM_S    0.006744\n",
      "9    DRINKNUM    0.006667\n",
      "10    PRETERM    0.006375\n",
      "11  HISPDAD_P    0.006235\n",
      "12     CIGNUM    0.005689\n",
      "13  HISPDAD_N    0.005365\n",
      "14   DIABETES    0.005237\n",
      "15   HEMOGLOB    0.005172\n",
      "16   LOUTCOME    0.004958\n",
      "17     VISITS    0.004913\n",
      "18    HYPERCH    0.004707\n",
      "19  HISPDAD_M    0.004629\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "target_variable = 'BWEIGHT'\n",
    "\n",
    "# Compute the Pearson Correlation Coefficients (PCC) between all variables and the target variable\n",
    "correlation_scores = X_train_ohe_imputed.corrwith(y_train)\n",
    "correlation_scores = abs(correlation_scores)\n",
    "print(correlation_scores)\n",
    "# Exclude the target variable from the list of highly correlated variables\n",
    "top20_corr_variables = correlation_scores.sort_values(ascending=False).head(20)\n",
    "top20_df = pd.DataFrame({'variable': top20_corr_variables.index, 'corr_score': top20_corr_variables.values})\n",
    "\n",
    "# Print the list of top 20 correlated variables\n",
    "print(\"List of top 20 correlated variables (excluding 'BWEIGHT'):\")\n",
    "print(top20_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 7: \n",
    "Given the X_train_ohe_imputed (as Pandas dataframe from task 5) and and top20_df (as Pandas Dataframe from Task 6) having 2 columns {variable_name,corr_score} similar to the one you computed in Task 6:\n",
    "* Please save as X_train_t20 keeping only the columns listed in the top20_df dataframe.\n",
    "* Repeat the process for X_test_ohe (obtained from task 4), and save it as X_test_t20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names of X_train_t20:\n",
      "['HISPDAD_O', 'HISPMOM_N', 'GAINED', 'HISPMOM_M', 'PINFANT', 'CERVIX', 'TERMS', 'MEDUC', 'HISPMOM_S', 'DRINKNUM', 'PRETERM', 'HISPDAD_P', 'CIGNUM', 'HISPDAD_N', 'DIABETES', 'HEMOGLOB', 'LOUTCOME', 'VISITS', 'HYPERCH', 'HISPDAD_M']\n",
      "\n",
      "Column names of X_test_t20:\n",
      "['HISPDAD_O', 'HISPMOM_N', 'GAINED', 'HISPMOM_M', 'PINFANT', 'CERVIX', 'TERMS', 'MEDUC', 'HISPMOM_S', 'DRINKNUM', 'PRETERM', 'HISPDAD_P', 'CIGNUM', 'HISPDAD_N', 'DIABETES', 'HEMOGLOB', 'LOUTCOME', 'VISITS', 'HYPERCH', 'HISPDAD_M']\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "\n",
    "top20_variables = top20_df['variable'].tolist()\n",
    "\n",
    "X_train_t20 = X_train_ohe_imputed[top20_variables]\n",
    "\n",
    "X_test_t20 = X_test_ohe[top20_variables]\n",
    "assert X_train_t20.columns.tolist() == X_test_t20.columns.tolist()\n",
    "\n",
    "print(\"Column names of X_train_t20:\")\n",
    "print(X_train_t20.columns.tolist())\n",
    "print(\"\\nColumn names of X_test_t20:\")\n",
    "print(X_test_t20.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 8: \n",
    "* Apply min-max scaling on the training dataset (X_train_t20 obtained from Task 7). Save the result as X_train_scaled_mm.\n",
    "* Then scale the test dataset (X_test_t20 obtained from Task 7) based on the metrics you obtain when you scale the training dataset. Save the result as X_test_scaled_mm.\n",
    "* PLEASE DO NOT SCALE y_train and y_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.         0.41836735 ... 0.36734694 0.         0.        ]\n",
      " [0.         1.         0.35714286 ... 0.2244898  0.         0.        ]\n",
      " [0.         1.         0.10204082 ... 0.24489796 0.         0.        ]\n",
      " ...\n",
      " [0.         1.         0.43877551 ... 0.30612245 0.         0.        ]\n",
      " [0.         1.         0.29591837 ... 0.30612245 0.         0.        ]\n",
      " [0.         1.         0.35714286 ... 0.40816327 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#X_scaled = (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled_mm = scaler.fit_transform(X_train_t20)\n",
    "X_test_scaled_mm = scaler.transform(X_test_t20)\n",
    "print(X_test_scaled_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 9: \n",
    "* Apply standardization (i.e., normalization) scaling on the training dataset (X_train_t20 obtained from Task 7). Save the result as X_train_scaled_std.\n",
    "* Then scale the test dataset (X_test_t20 obtained from Task 7) based on the metrics you obtain when you scale the training dataset. Save the result as X_test_scaled_std.\n",
    "* PLEASE DO NOT SCALE y_train and y_test.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03422943  0.43863282  0.78524868 ...  1.49974833 -0.11797833\n",
      "  -0.36727143]\n",
      " [-0.03422943  0.43863282  0.34460049 ... -0.38741983 -0.11797833\n",
      "  -0.36727143]\n",
      " [-0.03422943  0.43863282 -1.49143363 ... -0.11782438 -0.11797833\n",
      "  -0.36727143]\n",
      " ...\n",
      " [-0.03422943  0.43863282  0.93213141 ...  0.69096198 -0.11797833\n",
      "  -0.36727143]\n",
      " [-0.03422943  0.43863282 -0.0960477  ...  0.69096198 -0.11797833\n",
      "  -0.36727143]\n",
      " [-0.03422943  0.43863282  0.34460049 ...  2.03893924 -0.11797833\n",
      "  -0.36727143]]\n"
     ]
    }
   ],
   "source": [
    "#@TODO: Your code goes here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_std = scaler.fit_transform(X_train_t20)\n",
    "X_test_scaled_std = scaler.transform(X_test_t20)\n",
    "print(X_test_scaled_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 10: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_closed_form_training** : It fits a linear regression model using the closed-form solution to obtain the coefficients, beta's, as a numpy array of m+1 values (Please recall class lecture), where *m* is the number of variables kept in X_train (the first argument to the function). Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_closed_form_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function.\n",
    "* **RMSE**: It takes two lists: y_test, y_pred, where the first list represents ground truth (i.e., actual) target values for the given samples, and the second list represents a corresponding predicted values for exactly same number of samples in y_test. Compute and return the Root Mean Squared Error (RMSE) of the prediction. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_closed_form_training() function providing X_train_scaled_std, y_train obtained from Task 9, and save the returned results as betas_closed_form,cpu_time_closed_form.\n",
    "* Print betas_closed_form, cpu_time_closed_form\n",
    "* Call linear_regression_closed_form_predict() function providing betas_closed_form,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSE() function providing y_test and y_pred. Save returned result as rmse_closed_form.\n",
    "* Print rmse_closed_form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta values obtained using closed-form solution:\n",
      "[ 7.25851414e+00 -7.65031828e-03  3.79369226e-02  2.27356275e-01\n",
      "  1.23230197e-01  8.35106422e-02 -8.45235170e-02 -4.87993032e-02\n",
      "  6.79703755e-02  5.21196648e-02 -6.95468060e-03 -9.05090667e-02\n",
      " -1.38545340e-02 -9.04527213e-02 -3.48821163e-03  2.07455933e-02\n",
      " -1.13077027e-02 -1.03896253e-01  1.55884496e-01 -5.06580785e-02\n",
      "  1.12039360e-02]\n",
      "CPU time for closed-form training: 0.011653799999749026 seconds\n",
      "Predicted values (y_pred):\n",
      "[7.82785582 7.4566739  5.56372593 ... 7.68880833 7.5014103  7.77413762]\n",
      "RMSE (Root Mean Squared Error) using closed-form solution: 1.282413892855486\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from time import perf_counter\n",
    "\n",
    "def linear_regression_closed_form_training(X_train, y_train):\n",
    "    #@TODO: Your code goes here\n",
    "    # Add a column of ones to X_train for the intercept term\n",
    "    X_train_with_intercept = np.column_stack((np.ones(len(X_train)), X_train))\n",
    "    start_time = time.perf_counter()\n",
    "    betas = np.linalg.inv(X_train_with_intercept.T @ X_train_with_intercept) @ X_train_with_intercept.T @ y_train\n",
    "    end_time = time.perf_counter()\n",
    "    cpu_time = end_time - start_time\n",
    "    return betas, cpu_time\n",
    "\n",
    "# Call linear_regression_closed_form_training with X_train_scaled_std and y_train\n",
    "betas_closed_form, cpu_time_closed_form = linear_regression_closed_form_training(X_train_scaled_std, y_train)\n",
    "\n",
    "# Print the results\n",
    "print(\"Beta values obtained using closed-form solution:\")\n",
    "print(betas_closed_form)\n",
    "print(f\"CPU time for closed-form training: {cpu_time_closed_form} seconds\")\n",
    "\n",
    "\n",
    "def linear_regression_closed_form_predict(betas, X_test):    \n",
    "    #@TODO: Your code goes here\n",
    "    X_test_with_intercept = np.column_stack((np.ones(len(X_test)), X_test))\n",
    "    y_pred = X_test_with_intercept @ betas \n",
    "    return y_pred\n",
    "\n",
    "y_pred_closed_form = linear_regression_closed_form_predict(betas_closed_form, X_test_scaled_std)\n",
    "print(\"Predicted values (y_pred):\")\n",
    "print(y_pred_closed_form)\n",
    "\n",
    "def RMSE(y_test, y_pred_closed_form):    \n",
    "    #@TODO: Your code goes here\n",
    "    squared_errors = (y_test - y_pred_closed_form) ** 2\n",
    "    mean_squared_error = squared_errors.mean()\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    return rmse\n",
    "    \n",
    "rmse_closed_form = RMSE(y_test, y_pred_closed_form)\n",
    "print(f\"RMSE (Root Mean Squared Error) using closed-form solution: {rmse_closed_form}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 11: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_batch_training** : It fits a linear regression model using the batch gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate) and nEpoch (number of epochs) parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the cpu_time.\n",
    "* **linear_regression_gd_batch_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_batch_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.01,nEpoch=1000, and save the returned results as betas_batch,cpu_time_batch.\n",
    "* Print betas_batch, cpu_time_batch\n",
    "* Call linear_regression_gd_batch_predict() function providing betas_batch,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSE() function providing y_test and y_pred. Save returned result as rmse_batch.\n",
    "* Print rmse_batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta values obtained using batch gradient descent:\n",
      "[ 7.25851412e+00 -8.13788899e-03  3.54659413e-03  2.27245470e-01\n",
      "  8.93134805e-02  8.35576337e-02 -8.45560525e-02 -4.89423293e-02\n",
      "  6.73352035e-02  3.33207982e-02 -6.89686720e-03 -9.05937301e-02\n",
      " -1.61253312e-02 -9.05738922e-02 -3.74620448e-03  2.07485164e-02\n",
      " -1.13420079e-02 -1.04020366e-01  1.55849628e-01 -5.06674156e-02\n",
      "  1.45828936e-02]\n",
      "CPU time for batch gradient descent training: 1.704003400000147 seconds\n",
      "Predicted values (y_pred) using batch gradient descent:\n",
      "[7.82680662 7.45552269 5.56223281 ... 7.68820433 7.50048696 7.7727237 ]\n",
      "RMSE (Root Mean Squared Error) using gd-batch solution: 1.2825408999452304\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_gd_batch_training(X_train,y_train, alpha, nEpoch):\n",
    "    import random\n",
    "    random.seed(554433)\n",
    "    #@TODO: Your code goes here\n",
    "    X_train_with_intercept = np.column_stack((np.ones(len(X_train)), X_train))\n",
    "    betas = np.zeros(X_train_with_intercept.shape[1])\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch in range(nEpoch):\n",
    "        y_pred = X_train_with_intercept @ betas\n",
    "        gradient = -(2/len(X_train)) * (X_train_with_intercept.T @ (y_train - y_pred))\n",
    "        betas -= alpha * gradient\n",
    "    end_time = time.perf_counter()\n",
    "    cpu_time = end_time - start_time\n",
    "    return betas, cpu_time\n",
    "\n",
    "alpha = 0.01\n",
    "nEpoch = 1000\n",
    "betas_batch, cpu_time_batch = linear_regression_gd_batch_training(X_train_scaled_std, y_train, alpha, nEpoch)\n",
    "print(\"Beta values obtained using batch gradient descent:\")\n",
    "print(betas_batch)\n",
    "print(f\"CPU time for batch gradient descent training: {cpu_time_batch} seconds\")\n",
    "    \n",
    "def linear_regression_gd_batch_predict(betas,X_test):\n",
    "    #@TODO: Your code goes here\n",
    "    X_test_with_intercept = np.column_stack((np.ones(len(X_test)), X_test))\n",
    "    y_pred = X_test_with_intercept @ betas\n",
    "    return y_pred\n",
    "\n",
    "y_pred_gd_batch = linear_regression_gd_batch_predict(betas_batch, X_test_scaled_std)\n",
    "print(\"Predicted values (y_pred) using batch gradient descent:\")\n",
    "print(y_pred_gd_batch)\n",
    "    \n",
    "rmse_gd_batch = RMSE(y_test, y_pred_gd_batch)\n",
    "print(f\"RMSE (Root Mean Squared Error) using gd-batch solution: {rmse_gd_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 12:\n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_stochastic_training** : It fits a linear regression model using the stochastic gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate), nEpoch (number of epochs), nIteration (number of iterations) parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_gd_stochastic_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_stochastic_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.01,nEpoch=50, nIteration=100 , and save the returned results as betas_stochastic,cpu_time_stochastic.\n",
    "* Print betas_stochastic, cpu_time_stochastic\n",
    "* Call linear_regression_gd_stochastic_predict() function providing betas_stochastic,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSE() function providing y_test and y_pred. Save returned result as rmse_stochastic.\n",
    "* Print rmse_stochastic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta values obtained using stochastic gradient descent:\n",
      "[-8.23473841e+05 -1.20922537e+05 -7.52932647e+03 -1.75876076e+05\n",
      "  4.50967273e+06  1.10491607e+06 -5.60221588e+04  1.22679443e+04\n",
      "  2.13379456e+05 -6.35383479e+05  2.90577421e+04  2.14644689e+05\n",
      " -8.18363681e+04  8.46795845e+05 -6.51473779e+04  8.88219090e+05\n",
      "  2.28650880e+02 -5.88636608e+04  8.51897022e+03  7.54836487e+05]\n",
      "CPU time for stochastic gradient descent training: 0.836337100000037 seconds\n",
      "Predicted values (y_pred) using stochastic gradient descent:\n",
      "[ -380284.3947888   -261696.78961404 18036802.30321483 ...\n",
      "  -342150.58605964  -326040.70093096  -474283.57738614]\n",
      "RMSE (Root Mean Squared Error) using gd-stochastic solution: 4706519.621826915\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(554433)\n",
    "\n",
    "def linear_regression_gd_stochastic_training(X_train,y_train,alpha,nEpoch,nIteration):\n",
    "\n",
    "    ## YOUR CODE HERE ###\n",
    "    #X_train_with_intercept = np.column_stack((np.ones(len(X_train)), X_train))\n",
    "    betas = np.zeros(X_train.shape[1])\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(nEpoch):\n",
    "        for _ in range(nIteration):\n",
    "            random_index = random.randint(0, len(y_train) - 1)\n",
    "            xi = X_train[random_index:random_index+1]  \n",
    "            yi = y_train[random_index:random_index+1]\n",
    "            hi = np.dot(xi, betas)\n",
    "            loss = hi - yi\n",
    "            grad = xi.T.dot(loss)\n",
    "            betas-=alpha*grad\n",
    "    end_time = time.perf_counter()\n",
    "    cpu_time = end_time - start_time\n",
    "    return betas, cpu_time\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.01\n",
    "nEpoch = 50\n",
    "nIteration = 100\n",
    "betas_stochastic, cpu_time_stochastic = linear_regression_gd_stochastic_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration)\n",
    "print(\"Beta values obtained using stochastic gradient descent:\")\n",
    "print(betas_stochastic)\n",
    "print(f\"CPU time for stochastic gradient descent training: {cpu_time_stochastic} seconds\")\n",
    "    \n",
    "def linear_regression_gd_stochastic_predict(betas, X_test):\n",
    "    ## YOUR CODE HERE ###\n",
    "    y_pred = X_test @ betas\n",
    "    return y_pred\n",
    "\n",
    "y_pred_stochastic = linear_regression_gd_stochastic_predict(betas_stochastic, X_test_scaled_std)\n",
    "print(\"Predicted values (y_pred) using stochastic gradient descent:\")\n",
    "print(y_pred_stochastic)   \n",
    "\n",
    "rmse_gd_stochastic = RMSE(y_test, y_pred_stochastic)\n",
    "print(f\"RMSE (Root Mean Squared Error) using gd-stochastic solution: {rmse_gd_stochastic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 13: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_minibatch_training** : It fits a linear regression model using the minibatch gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate), nEpoch (number of epochs), nIteration (number of iterations), and batch_size parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_gd_minibatch_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_minibatch_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.001,nEpoch=50, nIteration=1000,batch_size=32, and save the returned results as betas_minibatch,cpu_time_minibatch.\n",
    "* Print betas_minibatch, cpu_time_minibatch\n",
    "* Call linear_regression_gd_minibatch_predict() function providing betas_minibatch,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSE() function providing y_test and y_pred. Save returned result as rmse_minibatch.\n",
    "* Print rmse_minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta values obtained using minibatch gradient descent:\n",
      "[ 0.00250937  0.00077206  0.1891885   0.00415452  0.03997162 -0.09501042\n",
      " -0.06103914  0.06528451 -0.03524226  0.03860145 -0.06915494  0.00999569\n",
      " -0.10551752  0.01523728  0.0185282   0.0365025  -0.07492871  0.13709662\n",
      " -0.05447477 -0.03314626]\n",
      "CPU time for minibatch gradient descent training: 8.052354800000103 seconds\n",
      "Predicted values (y_pred) using minibatch gradient descent:\n",
      "[ 0.5602041   0.24038055 -1.76847167 ...  0.43257792  0.28259108\n",
      "  0.49701631]\n",
      "RMSE (Root Mean Squared Error) using gd-minibatch solution: 7.37560292250942\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_gd_minibatch_training(X_train,y_train,alpha,nEpoch, nIteration, batch_size):\n",
    "    import random\n",
    "    random.seed(554433)\n",
    "\n",
    "    ## YOUR CODE HERE ###\n",
    "    #X_train_with_intercept = np.column_stack((np.ones(len(X_train)), X_train))\n",
    "    betas = np.zeros(X_train.shape[1])\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(nEpoch):\n",
    "        for _ in range(nIteration):\n",
    "            random_index = random.randint(0, len(y_train) - 1)\n",
    "            X_mini = X_train[random_index:random_index+1]  \n",
    "            y_mini = y_train[random_index:random_index+1]\n",
    "            y_pred = np.dot(X_mini, betas)\n",
    "            loss = y_pred - y_mini\n",
    "            grad = X_mini.T.dot(loss) / batch_size\n",
    "            betas-=alpha*grad\n",
    "            \n",
    "    end_time = time.perf_counter()\n",
    "    cpu_time = end_time - start_time\n",
    "    return betas, cpu_time\n",
    "    \n",
    "alpha = 0.001\n",
    "nEpoch = 50\n",
    "nIteration = 1000\n",
    "batch_size = 32\n",
    "betas_minibatch, cpu_time_minibatch = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "\n",
    "print(\"Beta values obtained using minibatch gradient descent:\")\n",
    "print(betas_minibatch)\n",
    "print(f\"CPU time for minibatch gradient descent training: {cpu_time_minibatch} seconds\")\n",
    "    \n",
    "def linear_regression_gd_minibatch_predict(betas,X_test):\n",
    "    ## YOUR CODE HERE ###\n",
    "    y_pred = X_test @ betas\n",
    "    return y_pred\n",
    "\n",
    "y_pred_minibatch = linear_regression_gd_stochastic_predict(betas_minibatch, X_test_scaled_std)\n",
    "print(\"Predicted values (y_pred) using minibatch gradient descent:\")\n",
    "print(y_pred_minibatch) \n",
    "    \n",
    "rmse_gd_minibatch = RMSE(y_test, y_pred_minibatch)\n",
    "print(f\"RMSE (Root Mean Squared Error) using gd-minibatch solution: {rmse_gd_minibatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 14:\n",
    "Given the 4 sets of results from the 4 experiments (from Tasks 10, 11, 12, 13) with closed form solution, batch gradient descent, stochastic gradient descent and mini-batch gradient descent, print a string from the set {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the best predictive performance in terms of RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best method in terms of RMSE is: closed-form\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "rmse_values = {\n",
    "    \"closed-form\": rmse_closed_form,\n",
    "    \"batch-GD\": rmse_gd_batch,\n",
    "    \"stochastic-GD\": rmse_gd_stochastic,\n",
    "    \"minibatch-GD\": rmse_gd_minibatch\n",
    "}\n",
    "\n",
    "best_method = min(rmse_values, key=rmse_values.get)\n",
    "print(f\"The best method in terms of RMSE is: {best_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 15: \n",
    "Given the 4 sets of results from the 4 experiments (from Tasks 10, 11, 12, 13) with closed form solution, batch gradient descent, stochastic gradient descent and mini-batch gradient descent, print a string from the set {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the least training cpu time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method with the least training CPU time is: closed-form\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "cpu_times = {\n",
    "    \"closed-form\": cpu_time_closed_form,\n",
    "    \"batch-GD\": cpu_time_batch,\n",
    "    \"stochastic-GD\": cpu_time_stochastic,\n",
    "    \"minibatch-GD\": cpu_time_minibatch\n",
    "}\n",
    "\n",
    "fastest_method = min(cpu_times, key=cpu_times.get)\n",
    "print(f\"The method with the least training CPU time is: {fastest_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 16: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively, \n",
    "* call your implementation of Task 12: stochastic gradient descent based linear regression for each of these learning rates: {0.0001, 0.001, 0.05, 0.01, 0.1, 1.0}\n",
    "    * Please use the nIteration (number of iterations), nEpoch (number of epoch) parameters in your implementation of the gradient descent algorithm.\n",
    "\n",
    "* For each of the linear regression model, using the computed beta values, predict the test samples provided in the \"X_test_scaled_std\" argument, and let's name your prediction \"y_pred\".\n",
    "* Compute Root Mean Squared Error (RMSE) of your prediction using the RMSE() function you defined in Task 10.\n",
    "* Finally, print the learning rate that shows the best test performance, and also print as a pandas dataframe named summary with 2 columns: {learning_rate, test_RMSE} containing RMSE's of the 6 linear regression models. Also, print the best performing learning rate.\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_25924\\4042095219.py:18: RuntimeWarning: invalid value encountered in subtract\n",
      "  betas-=alpha*grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing learning rate is: 0.0001\n",
      "   learning_rate     test_RMSE\n",
      "0         0.0001  7.376127e+00\n",
      "1         0.0010  7.423693e+00\n",
      "2         0.0100  1.671860e+58\n",
      "3         0.0500           NaN\n",
      "4         0.1000           NaN\n",
      "5         1.0000           NaN\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.05, 0.1, 1.0]\n",
    "\n",
    "best_learning_rate = None\n",
    "best_test_rmse = float('inf')\n",
    "\n",
    "rmse_values = []\n",
    "\n",
    "nEpoch = 50\n",
    "nIteration = 1000\n",
    "\n",
    "# Iterate over the learning rates\n",
    "for alpha in learning_rates:\n",
    "    betas, _ = linear_regression_gd_stochastic_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration)\n",
    "    y_pred = linear_regression_gd_stochastic_predict(betas, X_test_scaled_std)\n",
    "    \n",
    "    # Calculate the test RMSE\n",
    "    test_rmse = RMSE(y_test, y_pred)\n",
    "    rmse_values.append(test_rmse)\n",
    "    \n",
    "    if test_rmse < best_test_rmse:\n",
    "        best_test_rmse = test_rmse\n",
    "        best_learning_rate = alpha\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'learning_rate': learning_rates,\n",
    "    'test_RMSE': rmse_values\n",
    "})\n",
    "\n",
    "print(f\"The best performing learning rate is: {best_learning_rate}\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 17:\n",
    "* Utilizing the best trained linear regression model (so far), predict the target for each of the samples in the `judge-without-labels.csv` dataset.\n",
    "    * I believe you will not forget to do the following before calling the prediction algorithm:\n",
    "        - Save the ID values of the judge dataset into ID_judge and drop it from the judge dataframe.\n",
    "        - Perform onehot encoding using the same encoder you used to encode X_test (Task 4). \n",
    "        - keep only the same top 20 variables as you did in Task 7. \n",
    "        - scale the input variables based on the same metrics you used to scale the training dataset (Task 9). \n",
    "    * Now, call the prediction function of that model to obtain y_pred.\n",
    "    * Prepare and print as a pandas dataframe having columns: {ID, BWEIGHT}, where ID will the ID of the judge sample, and BWEIGHT is the corresponding y_pred value from your model prediction.\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SEX', 'MARITAL', 'FAGE', 'GAINED', 'VISITS', 'MAGE', 'FEDUC', 'MEDUC',\n",
      "       'TOTALP', 'BDEAD', 'TERMS', 'LOUTCOME', 'WEEKS', 'RACEMOM', 'RACEDAD',\n",
      "       'CIGNUM', 'DRINKNUM', 'ANEMIA', 'CARDIAC', 'ACLUNG', 'DIABETES',\n",
      "       'HERPES', 'HYDRAM', 'HEMOGLOB', 'HYPERCH', 'HYPERPR', 'ECLAMP',\n",
      "       'CERVIX', 'PINFANT', 'PRETERM', 'RENAL', 'RHSEN', 'UTERINE',\n",
      "       'HISPMOM_M', 'HISPMOM_N', 'HISPMOM_O', 'HISPMOM_P', 'HISPMOM_S',\n",
      "       'HISPDAD_C', 'HISPDAD_M', 'HISPDAD_N', 'HISPDAD_O', 'HISPDAD_P',\n",
      "       'HISPDAD_S', 'HISPDAD_U'],\n",
      "      dtype='object')\n",
      "      GAINED  MAGE  FEDUC  RENAL  MARITAL  TERMS  CIGNUM  FAGE  HISPMOM_S  \\\n",
      "0         70    26     14      0        1      3       0    30          0   \n",
      "1         36    18      9      0        2      0       0    21          0   \n",
      "2         18    25     12      0        1      3      20    22          1   \n",
      "3         25    22     12      0        1      0       0    24          0   \n",
      "4         38    26     12      0        1      0       0    24          0   \n",
      "...      ...   ...    ...    ...      ...    ...     ...   ...        ...   \n",
      "1995      17    21     13      0        1      0       0    21          0   \n",
      "1996      20    26     16      1        1      0       0    27          0   \n",
      "1997      25    27     12      0        1      0       0    29          0   \n",
      "1998      29    34     12      0        1      0       0    37          0   \n",
      "1999      20    27     12      0        2      0       5    30          0   \n",
      "\n",
      "      SEX  CERVIX  HISPMOM_N  ACLUNG  HISPDAD_N  HISPDAD_C  HISPMOM_P  \\\n",
      "0       1       0          1       0          1          0          0   \n",
      "1       2       0          1       1          1          0          0   \n",
      "2       2       0          0       1          1          0          0   \n",
      "3       2       0          1       0          1          0          0   \n",
      "4       2       0          1       0          1          0          0   \n",
      "...   ...     ...        ...     ...        ...        ...        ...   \n",
      "1995    2       0          1       0          1          0          0   \n",
      "1996    1       0          1       0          1          0          0   \n",
      "1997    1       0          1       0          1          0          0   \n",
      "1998    1       0          1       0          1          0          0   \n",
      "1999    1       0          1       0          1          0          0   \n",
      "\n",
      "      HISPDAD_P  UTERINE  HISPDAD_S  PINFANT  \n",
      "0             0        0          0        0  \n",
      "1             0        0          0        0  \n",
      "2             0        0          0        0  \n",
      "3             0        0          0        0  \n",
      "4             0        0          0        0  \n",
      "...         ...      ...        ...      ...  \n",
      "1995          0        0          0        0  \n",
      "1996          0        0          0        0  \n",
      "1997          0        0          0        0  \n",
      "1998          0        0          0        0  \n",
      "1999          0        0          0        0  \n",
      "\n",
      "[2000 rows x 20 columns]\n",
      "[[ 2.90008064 -0.27315798  0.38477065 ... -0.05006262 -0.18903116\n",
      "  -0.06723349]\n",
      " [ 0.41647821 -1.60971081 -1.36418685 ... -0.05006262 -0.18903116\n",
      "  -0.06723349]\n",
      " [-0.89837013 -0.44022709 -0.31481235 ... -0.05006262 -0.18903116\n",
      "  -0.06723349]\n",
      " ...\n",
      " [-0.38704022 -0.10608888 -0.31481235 ... -0.05006262 -0.18903116\n",
      "  -0.06723349]\n",
      " [-0.0948517   1.06339484 -0.31481235 ... -0.05006262 -0.18903116\n",
      "  -0.06723349]\n",
      " [-0.75227587 -0.10608888 -0.31481235 ... -0.05006262 -0.18903116\n",
      "  -0.06723349]]\n",
      "        ID    BWEIGHT\n",
      "0        1   7.011698\n",
      "1        2   6.144393\n",
      "2        3   5.978701\n",
      "3        4   7.092773\n",
      "4        5   7.110860\n",
      "...    ...        ...\n",
      "1995  1996   7.140788\n",
      "1996  1997  10.666763\n",
      "1997  1998   7.187781\n",
      "1998  1999   7.308964\n",
      "1999  2000   7.305681\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "judge_df = pd.read_csv('dataset/judge-without-labels.csv',delimiter=',')\n",
    "\n",
    "# Save the ID values and drop the ID column\n",
    "ID_judge = judge_df['ID'].tolist()\n",
    "ID_judge_series = pd.Series(ID_judge)\n",
    "judge_df.drop(columns=['ID'], inplace=True)\n",
    " \n",
    "# # Perform one-hot encoding on the judge dataset\n",
    "judge_ohe = pd.get_dummies(judge_df)\n",
    "print(judge_ohe.columns)\n",
    "\n",
    "correlation = judge_ohe.corrwith(ID_judge_series).abs().sort_values(ascending=False).head(20)\n",
    "\n",
    "judge_t20 = pd.DataFrame({'variable': correlation.index, 'corr_score': correlation.values})\n",
    "top20_variables = judge_t20['variable'].tolist()\n",
    "\n",
    "judge_train_t20 = judge_ohe[top20_variables]\n",
    "print(judge_train_t20)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "judge_scaled_std = scaler.fit_transform(judge_train_t20)\n",
    "print(judge_scaled_std)\n",
    "\n",
    "y_pred_judge = linear_regression_closed_form_predict(betas_closed_form, judge_scaled_std)\n",
    "\n",
    "result = pd.DataFrame({'ID': ID_judge, 'BWEIGHT': y_pred_judge})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 18:\n",
    "* Bring your best model and submit your solution as a csv file at [http://10.18.108.99/fa23-ml-check-PA1-judge-pred.php](http://10.18.108.99/fa23-ml-check-PA1-judge-pred.php). Please note this is hosted within the campus network. So, if you want to access it from outside campus, you need to first establish a VPN connection.\n",
    "  * Make sure your csv file should have the following format and have exactly the same number of samples as in the provided `judge-without-labels.csv` file:\n",
    "\n",
    "```\n",
    "ID,BWEIGHT\n",
    "1,10.1875\n",
    "2,6.625\n",
    "3,7.0625\n",
    "...\n",
    "...\n",
    "``` \n",
    "\n",
    "\n",
    "* Can you do better?\n",
    "  * Demonstrate your effort by pushing yourself to improve your own score or beat other submissions (if any available) until the deadline and document your scores and list what changes you made in your submitted solution.\n",
    "\n",
    "* Please report below all your RMSLE scores for each of your submission (one per line). You may enumerate your submissions by 1, 2, 3, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ### Adaptive Grading\n",
    "def adagrad_training(X_train, y_train, learning_rate, nEpoch, nIteration):\n",
    "    \n",
    "    betas = np.zeros(X_train.shape[1] + 1) \n",
    "    accumulated_gradients = np.zeros(X_train.shape[1] + 1)  \n",
    "\n",
    "    for epoch in range(nEpoch):\n",
    "        for iteration in range(nIteration):\n",
    "            \n",
    "            gradients = gradientsCompute(X_train, y_train, betas)\n",
    "            accumulated_gradients += gradients ** 2\n",
    "            betas -= learning_rate * gradients / np.sqrt(accumulated_gradients + 1e-8)\n",
    "\n",
    "    return betas\n",
    "\n",
    "def gradientsCompute(X, y, betas):\n",
    "    y_pred = np.dot(X, betas[1:]) + betas[0]\n",
    "    \n",
    "    error = y_pred - y\n",
    "    \n",
    "    gradients = np.zeros_like(betas)\n",
    "    gradients[0] = 2 * np.sum(error) \n",
    "    gradients[1:] = 2 * np.dot(X.T, error) \n",
    "\n",
    "    return gradients\n",
    "\n",
    "def adagrad_predict(betas, X_test):\n",
    "    y_pred = np.dot(X_test, betas[1:]) + betas[0]\n",
    "    return y_pred\n",
    "\n",
    "def RMSLE(y_true, y_pred):\n",
    "    log_diff = np.log(y_true + 1) - np.log(y_pred + 1)\n",
    "    squared_log_diff = log_diff ** 2\n",
    "    mean_squared_log_diff = np.mean(squared_log_diff)\n",
    "    rmsle = np.sqrt(mean_squared_log_diff)\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Adagrad): 0.11610039005246287 \n",
      "\n",
      "RMSLE (Adagrad): 0.014201777466582874 \n",
      " RMSLE (Minibatch): 2.199423656633212 \n",
      " RMSLE (Stochastic): 12.42031070813986 \n",
      " RMSLE (batch): 0.18631749069958226 \n",
      " RMSLE (closed_form): 0.1862986220630959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_25924\\3354250080.py:32: RuntimeWarning: invalid value encountered in log\n",
      "  log_diff = np.log(y_true + 1) - np.log(y_pred + 1)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "nEpoch = 50\n",
    "nIteration = 100\n",
    "\n",
    "betas_adagrad = adagrad_training(X_train_scaled_std, y_train, learning_rate, nEpoch, nIteration)\n",
    "y_judge = result['BWEIGHT']\n",
    "\n",
    "y_pred_adagrad_judge = adagrad_predict(betas_adagrad, judge_scaled_std)\n",
    "\n",
    "rmse_adagrad = RMSE(y_judge, y_pred_adagrad_judge)\n",
    "\n",
    "print(\"RMSE (Adagrad):\", rmse_adagrad,\"\\n\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame({'ID': range(1, len(y_pred_adagrad_judge) + 1), 'BWEIGHT': y_pred_adagrad_judge})\n",
    "submission_df.to_csv('ada_submission.csv', index=False)\n",
    "\n",
    "rmsle_adagrad = RMSLE(y_judge, y_pred_adagrad_judge)\n",
    "rmsle_minibatch = RMSLE(y_test, y_pred_minibatch)\n",
    "rmsle_stochastic = RMSLE(y_test, y_pred_stochastic)\n",
    "rmsle_batch = RMSLE(y_test, y_pred_gd_batch)\n",
    "rmsle_closed_form = RMSLE(y_test, y_pred_closed_form)\n",
    "print(\"RMSLE (Adagrad):\", rmsle_adagrad, \"\\n\" ,\n",
    "      \"RMSLE (Minibatch):\", rmsle_minibatch, \"\\n\" ,\n",
    "      \"RMSLE (Stochastic):\", rmsle_stochastic, \"\\n\",\n",
    "      \"RMSLE (batch):\", rmsle_batch, \"\\n\",\n",
    "      \"RMSLE (closed_form):\", rmsle_closed_form     \n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
